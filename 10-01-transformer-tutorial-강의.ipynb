{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"10-01-transformer-tutorial-강의.ipynb","provenance":[],"collapsed_sections":["ObWGnfj6Ic_M","MH_M42l5Iezo","Z4OCTlKPIh0Z","zv8egauKIl6h","sgmPfzaBIo_V"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"JHkHg6XAXoyK"},"source":["# Evn"]},{"cell_type":"code","metadata":{"id":"WkYXFwcBXJDG"},"source":["import os\n","import random\n","import shutil\n","import json\n","import zipfile\n","import math\n","import copy\n","import collections\n","import re\n","\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow.keras.backend as KK\n","\n","from tqdm.notebook import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nvjyruUlXtlR"},"source":["# random seed initialize\n","random_seed = 1234\n","random.seed(random_seed)\n","np.random.seed(random_seed)\n","tf.random.set_seed(random_seed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BC3fXkhdYcYt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612754257761,"user_tz":-540,"elapsed":2277,"user":{"displayName":"한백규","photoUrl":"","userId":"00157431602964203104"}},"outputId":"a67dd648-5b94-48e0-d865-743be4462357"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pKEZAHeCzxRm"},"source":["# Config"]},{"cell_type":"code","metadata":{"id":"u9VJvPSx12Q8"},"source":["class Config(dict):\n","    \"\"\"\n","    json을 config 형태로 사용하기 위한 Class\n","    :param dict: config dictionary\n","    \"\"\"\n","    __getattr__ = dict.__getitem__\n","    __setattr__ = dict.__setitem__\n","\n","    @classmethod\n","    def load(cls, file):\n","        \"\"\"\n","        file에서 Config를 생성 함\n","        :param file: filename\n","        \"\"\"\n","        with open(file, 'r') as f:\n","            config = json.loads(f.read())\n","            return Config(config)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Bb_Q7DhzlGp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612754263100,"user_tz":-540,"elapsed":813,"user":{"displayName":"한백규","photoUrl":"","userId":"00157431602964203104"}},"outputId":"b7a4fbac-5399-4d2c-9639-83a8891aac52"},"source":["# config 생성\n","# d_model: model hidden dim\n","# n_head: multi head attention head number\n","# d_head: multi head attention head dim\n","# dropout: dropout rate\n","# d_ff: feed forward dim\n","# norm_eps: layernormal epsilon\n","# n_layer: layer number\n","# n_seq: sequence max number\n","# n_vocab: vocab count\n","# i_pad: vocab pad id\n","config = Config({\"d_model\": 8,\n","                 \"n_head\": 2,\n","                 \"d_head\": 4,\n","                 \"dropout\": 0.1,\n","                 \"d_ff\": 32,\n","                 \"norm_eps\": 0.001,\n","                 \"n_layer\": 6,\n","                 \"n_seq\": 16,\n","                 \"n_vocab\": 16,\n","                 \"i_pad\": 0})\n","config"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'d_ff': 32,\n"," 'd_head': 4,\n"," 'd_model': 8,\n"," 'dropout': 0.1,\n"," 'i_pad': 0,\n"," 'n_head': 2,\n"," 'n_layer': 6,\n"," 'n_seq': 16,\n"," 'n_vocab': 16,\n"," 'norm_eps': 0.001}"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"sDYpkXKSa0S0"},"source":["# Input"]},{"cell_type":"code","metadata":{"id":"bD4ou2ahas6z"},"source":["# 입력 문장\n","sentences = [\n","    ['나는 오늘 행복해', '나도 기분이 좋아'],\n","    # ['나는 오늘 기분이 좋아', '나도 매우 행복하다'],\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gVax4yWKa6iG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612754504420,"user_tz":-540,"elapsed":786,"user":{"displayName":"한백규","photoUrl":"","userId":"00157431602964203104"}},"outputId":"987dd6fd-f2a7-4ed4-94ff-d38901d74302"},"source":["# 각 문장을 띄어쓰기 단위로 분할\n","words = []\n","for pair in sentences:\n","    for sentence in pair:\n","        words.extend(sentence.split())\n","\n","# 중복 단어 제거\n","words = list(dict.fromkeys(words))\n","\n","# 각 단어별 고유한 번호 부여\n","word_to_id = {'[PAD]': 0, '[UNK]': 1, '[BOS]': 2, '[EOS]': 3}\n","for word in words:\n","    word_to_id[word] = len(word_to_id)\n","\n","# 각 숫자별 단어 부여\n","id_to_word = {_id:word for word, _id in word_to_id.items()}\n","\n","word_to_id, id_to_word"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["({'[BOS]': 2,\n","  '[EOS]': 3,\n","  '[PAD]': 0,\n","  '[UNK]': 1,\n","  '기분이': 8,\n","  '나는': 4,\n","  '나도': 7,\n","  '오늘': 5,\n","  '좋아': 9,\n","  '행복해': 6},\n"," {0: '[PAD]',\n","  1: '[UNK]',\n","  2: '[BOS]',\n","  3: '[EOS]',\n","  4: '나는',\n","  5: '오늘',\n","  6: '행복해',\n","  7: '나도',\n","  8: '기분이',\n","  9: '좋아'})"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"ooN0V7loa8a9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612754509026,"user_tz":-540,"elapsed":769,"user":{"displayName":"한백규","photoUrl":"","userId":"00157431602964203104"}},"outputId":"d8a4bbb9-0b85-4593-e0c5-e2c8bce85315"},"source":["# Question과 Answer를 숫자료\n","question_list, answer_list = [], []\n","\n","for pair in sentences:\n","    question_list.append([word_to_id[word] for word in pair[0].split()])\n","    answer_list.append([word_to_id[word] for word in pair[1].split()])\n","\n","# 학습용 입력 데이터 생성\n","train_enc_inputs, train_dec_inputs, train_labels = [], [], []\n","for question, answer in zip(question_list, answer_list):\n","    train_enc_inputs.append(question)\n","    train_dec_inputs.append([word_to_id['[BOS]']] + answer)\n","    train_labels.append(answer + [word_to_id['[EOS]']])\n","\n","# Encoder 입력의 길이를 모두 동일하게 변경 (최대길이 4)\n","for row in train_enc_inputs:\n","    row += [0] * (4 - len(row))\n","\n","# Decoder 입력의 길이를 모두 동일하게 변경 (최대길이 6)\n","for row in train_dec_inputs:\n","    row += [0] * (6 - len(row))\n","\n","# 정답의 길이를 모두 동일하게 변경 (최대길이 6)\n","for row in train_labels:\n","    row += [0] * (6 - len(row))\n","\n","# numpy array로 변환/\n","train_enc_inputs = np.array(train_enc_inputs)\n","train_dec_inputs = np.array(train_dec_inputs)\n","train_labels = np.array(train_labels)\n","\n","train_enc_inputs, train_dec_inputs, train_labels"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[4, 5, 6, 0]]),\n"," array([[2, 7, 8, 9, 0, 0]]),\n"," array([[7, 8, 9, 3, 0, 0]]))"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"5hpmht2zeTC9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612754540598,"user_tz":-540,"elapsed":778,"user":{"displayName":"한백규","photoUrl":"","userId":"00157431602964203104"}},"outputId":"8d48889d-d7b9-4670-ca3a-43b273d773d3"},"source":["# embedding with random weight\n","embed_weight = np.random.randint(-9, 10, (config.n_vocab, config.d_model)) / 10\n","\n","embed = tf.keras.layers.Embedding(config.n_vocab, config.d_model, weights=[embed_weight])\n","embed_weight"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.6, -0.3,  0.3,  0.6,  0.8,  0. ,  0.2,  0.3],\n","       [ 0.7, -0.4,  0.7,  0. ,  0.6,  0.9,  0.7,  0.3],\n","       [-0.4, -0.7, -0.3, -0.6, -0.2,  0.2, -0.9,  0. ],\n","       [ 0.2,  0.7, -0.6, -0.7,  0.3, -0.8,  0.2,  0.2],\n","       [ 0.8,  0.5, -0.2,  0.1,  0.2,  0.5,  0.8,  0.4],\n","       [-0.9,  0.3, -0.4,  0.8, -0.4,  0.4,  0.7,  0. ],\n","       [-0.1,  0.3, -0.3,  0.3,  0.6,  0.8,  0.9,  0.5],\n","       [-0.7, -0.4,  0.4, -0.3, -0.2, -0.5, -0.6, -0.4],\n","       [ 0.5,  0.6,  0.6,  0.6, -0.7,  0.1, -0.5,  0.9],\n","       [-0.2,  0.2,  0.5,  0.9,  0. , -0.9, -0.7, -0.8],\n","       [ 0.9,  0.8, -0.2, -0.5, -0.2,  0.8, -0.9,  0. ],\n","       [ 0.9,  0. , -0.8,  0.5, -0.6,  0.3,  0. ,  0.4],\n","       [-0.9, -0.5, -0.5, -0.9, -0.1,  0.3,  0.8,  0. ],\n","       [ 0.6, -0.1, -0.7,  0.7,  0.2, -0.7,  0.9,  0.6],\n","       [-0.6,  0.5, -0.7, -0.5, -0.8,  0.1, -0.7,  0.4],\n","       [-0.6,  0.9,  0.3, -0.4,  0.8,  0.7, -0.1,  0.4]])"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"SOj4JVDTe2Ct","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612754544616,"user_tz":-540,"elapsed":846,"user":{"displayName":"한백규","photoUrl":"","userId":"00157431602964203104"}},"outputId":"f4450e4a-ce8f-4d7a-c609-41413ec25587"},"source":["# encoder hidden\n","hidden_enc = embed(train_enc_inputs)\n","hidden_enc"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 4, 8), dtype=float32, numpy=\n","array([[[ 0.8,  0.5, -0.2,  0.1,  0.2,  0.5,  0.8,  0.4],\n","        [-0.9,  0.3, -0.4,  0.8, -0.4,  0.4,  0.7,  0. ],\n","        [-0.1,  0.3, -0.3,  0.3,  0.6,  0.8,  0.9,  0.5],\n","        [ 0.6, -0.3,  0.3,  0.6,  0.8,  0. ,  0.2,  0.3]]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"FJWefv-5e-wu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612754546565,"user_tz":-540,"elapsed":808,"user":{"displayName":"한백규","photoUrl":"","userId":"00157431602964203104"}},"outputId":"eb34956f-e756-4f00-a2cf-e91ceacff56e"},"source":["# decoder hidden\n","hidden_dec = embed(train_dec_inputs)\n","hidden_dec"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 6, 8), dtype=float32, numpy=\n","array([[[-0.4, -0.7, -0.3, -0.6, -0.2,  0.2, -0.9,  0. ],\n","        [-0.7, -0.4,  0.4, -0.3, -0.2, -0.5, -0.6, -0.4],\n","        [ 0.5,  0.6,  0.6,  0.6, -0.7,  0.1, -0.5,  0.9],\n","        [-0.2,  0.2,  0.5,  0.9,  0. , -0.9, -0.7, -0.8],\n","        [ 0.6, -0.3,  0.3,  0.6,  0.8,  0. ,  0.2,  0.3],\n","        [ 0.6, -0.3,  0.3,  0.6,  0.8,  0. ,  0.2,  0.3]]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"BHIf3KEV09K3"},"source":["# Mask"]},{"cell_type":"markdown","metadata":{"id":"DMLh4TXbGrHf"},"source":["## PAD Mask"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h8_CBNkQZdci","executionInfo":{"status":"ok","timestamp":1612754552050,"user_tz":-540,"elapsed":781,"user":{"displayName":"한백규","photoUrl":"","userId":"00157431602964203104"}},"outputId":"23587e7e-02c0-41e7-da48-8ed9340cb435"},"source":["train_enc_inputs, train_dec_inputs"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[4, 5, 6, 0]]), array([[2, 7, 8, 9, 0, 0]]))"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"ctSRBdAeG7xd"},"source":["def get_pad_mask(tokens, i_pad=0):\n","    \"\"\"\n","    pad mask 계산하는 함수\n","    :param tokens: tokens (bs, n_seq)\n","    :param i_pad: id of pad\n","    :return mask: pad mask (pad: 1, other: 0)\n","    \"\"\"\n","    #########################################\n","    # 0인 부분 확인\n","    mask = tf.math.equal(tokens, i_pad)\n","    # boolean -> float 32\n","    mask = tf.cast(mask, tf.float32)\n","    # expand dimension for n_seq\n","    mask = tf.expand_dims(mask, axis=1)\n","    #########################################\n","    return mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GqJIRPejHOuX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612754555697,"user_tz":-540,"elapsed":839,"user":{"displayName":"한백규","photoUrl":"","userId":"00157431602964203104"}},"outputId":"975b89cf-ad8e-475e-d78d-dc99adbdaa1d"},"source":["enc_pad_mask = get_pad_mask(train_enc_inputs)\n","enc_pad_mask"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 1, 4), dtype=float32, numpy=array([[[0., 0., 0., 1.]]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"AP6eq87QHWHv"},"source":["## Causal Mask"]},{"cell_type":"code","metadata":{"id":"jgCB3zk-HykT"},"source":["def get_causal_mask(tokens, i_pad=0):\n","    \"\"\"\n","    causal mask 계산하는 함수\n","    :param tokens: tokens (bs, n_seq)\n","    :param i_pad: id of pad\n","    :return mask: causal and pad mask (causal or pad: 1, other: 0)\n","    \"\"\"\n","    #########################################\n","    # 개수 조회\n","    n_seq = tf.shape(tokens)[1]\n","    # print(n_seq)\n","    # make ahead mask\n","    mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n","    # expand dim for bs\n","    mask = tf.expand_dims(mask, axis=0)\n","    # print(mask)\n","    # get pad_mask\n","    pad_mask = get_pad_mask(tokens, i_pad)\n","    # print(pad_mask)\n","    # mask all ahead_mask or pad_mask\n","    mask = tf.maximum(mask, pad_mask)\n","    #########################################\n","    return mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gUsByA_eH296","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612754559301,"user_tz":-540,"elapsed":813,"user":{"displayName":"한백규","photoUrl":"","userId":"00157431602964203104"}},"outputId":"49da80c2-dc7c-4806-98ed-d9ff02b8e119"},"source":["dec_causal_mask = get_causal_mask(train_dec_inputs)\n","dec_causal_mask"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 6, 6), dtype=float32, numpy=\n","array([[[0., 1., 1., 1., 1., 1.],\n","        [0., 0., 1., 1., 1., 1.],\n","        [0., 0., 0., 1., 1., 1.],\n","        [0., 0., 0., 0., 1., 1.],\n","        [0., 0., 0., 0., 1., 1.],\n","        [0., 0., 0., 0., 1., 1.]]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"itVetPQs1D98"},"source":["## Mask 생성"]},{"cell_type":"code","metadata":{"id":"BRfuGqhx1Ddw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612754561114,"user_tz":-540,"elapsed":811,"user":{"displayName":"한백규","photoUrl":"","userId":"00157431602964203104"}},"outputId":"bc20d961-6355-4c34-dcb3-eb3974998293"},"source":["# Encoder Self Attetnion mask\n","enc_self_mask = get_pad_mask(train_enc_inputs)\n","enc_self_mask"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 1, 4), dtype=float32, numpy=array([[[0., 0., 0., 1.]]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"bGIvBeLy1C5P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612754563134,"user_tz":-540,"elapsed":777,"user":{"displayName":"한백규","photoUrl":"","userId":"00157431602964203104"}},"outputId":"cf31728e-feed-4b12-f7d9-237b531bc51e"},"source":["# Decoder Self Attetnion mask\n","dec_self_mask = get_causal_mask(train_dec_inputs)\n","dec_self_mask"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 6, 6), dtype=float32, numpy=\n","array([[[0., 1., 1., 1., 1., 1.],\n","        [0., 0., 1., 1., 1., 1.],\n","        [0., 0., 0., 1., 1., 1.],\n","        [0., 0., 0., 0., 1., 1.],\n","        [0., 0., 0., 0., 1., 1.],\n","        [0., 0., 0., 0., 1., 1.]]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"MbU42aps1Vy3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612754565222,"user_tz":-540,"elapsed":811,"user":{"displayName":"한백규","photoUrl":"","userId":"00157431602964203104"}},"outputId":"80f6a0f7-34ca-4657-e590-05966507172e"},"source":["# Encoder-Decoder Attetnion mask\n","enc_dec_mask = get_pad_mask(train_enc_inputs)\n","enc_dec_mask"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 1, 4), dtype=float32, numpy=array([[[0., 0., 0., 1.]]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"A_kvndtnH-gh"},"source":["# Scaled dot product attention"]},{"cell_type":"code","metadata":{"id":"IFkp9rLd_Etz"},"source":["class ScaleDotProductAttention(tf.keras.layers.Layer):\n","    \"\"\"\n","    Scale Dot Product Attention Class\n","    \"\"\"\n","    def __init__(self, name=\"scale_dot_product_attention\"):\n","        \"\"\"\n","        생성자\n","        :param name: layer name\n","        \"\"\"\n","        super().__init__(name=name)\n","\n","    def call(self, inputs):\n","        \"\"\"\n","        layer 실행\n","        :param inputs: Q, K, V, attn_mask tuple\n","        :return attn_out: attention 실행 결과\n","        \"\"\"\n","        #########################################\n","        Q, K, V, attn_mask = inputs\n","        # matmul Q, K (transpose_b=True)\n","        attn_score = tf.matmul(Q, K, transpose_b=True)\n","        # get scale = d_model ** 0.5\n","        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n","        # print(attn_score)\n","        # divide by scale\n","        attn_scale = tf.math.divide(attn_score, scale)\n","        # print(attn_scale)\n","        # do mask (subtract 1e-9 for masked value)\n","        attn_scale -= 1.e9 * attn_mask\n","        # print(attn_scale)\n","        # calculate attention prob\n","        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n","        # print(attn_prob)\n","        # weighted sum of V\n","        attn_out = tf.matmul(attn_prob, V)\n","        return attn_out\n","        #########################################"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QlFeIqeUAlOd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612754572980,"user_tz":-540,"elapsed":777,"user":{"displayName":"한백규","photoUrl":"","userId":"00157431602964203104"}},"outputId":"9bdd80f6-8370-41d4-b751-bb7296a0fe18"},"source":["# Encoder Self Attetnion\n","Q = hidden_enc\n","K = hidden_enc\n","V = hidden_enc\n","\n","attention = ScaleDotProductAttention()\n","attn_out = attention((Q, K, V, enc_self_mask))\n","attn_out"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 4, 8), dtype=float32, numpy=\n","array([[[ 0.09644104,  0.383483  , -0.28066254,  0.32853726,\n","          0.20899351,  0.5851593 ,  0.81345034,  0.34623823],\n","        [-0.28741056,  0.34327316, -0.3261309 ,  0.49556422,\n","          0.03577897,  0.54402035,  0.7828285 ,  0.23952606],\n","        [-0.03932257,  0.36284068, -0.29634288,  0.37597537,\n","          0.19668652,  0.5946861 ,  0.81305325,  0.3297636 ],\n","        [ 0.0439067 ,  0.3756832 , -0.28674185,  0.3472341 ,\n","          0.2027991 ,  0.5881414 ,  0.8129915 ,  0.33924115]]],\n","      dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"pU-46iGWeIM3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612754574661,"user_tz":-540,"elapsed":829,"user":{"displayName":"한백규","photoUrl":"","userId":"00157431602964203104"}},"outputId":"878f65a1-fe61-45cd-b63f-cefd598ca25a"},"source":["# Decoder Self Attetnion\n","Q = hidden_dec\n","K = hidden_dec\n","V = hidden_dec\n","\n","attn_out = attention((Q, K, V, dec_self_mask))\n","attn_out"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 6, 8), dtype=float32, numpy=\n","array([[[-0.4       , -0.7       , -0.3       , -0.6       ,\n","         -0.2       ,  0.2       , -0.9       ,  0.        ],\n","        [-0.5661127 , -0.5338873 ,  0.08759623, -0.43388736,\n","         -0.2       , -0.18759623, -0.7338874 , -0.22148357],\n","        [ 0.1065836 ,  0.17015488,  0.39534226,  0.20757586,\n","         -0.5128951 ,  0.00528993, -0.5930563 ,  0.48770773],\n","        [-0.22024183,  0.00745238,  0.3843947 ,  0.36241618,\n","         -0.19720611, -0.47977903, -0.6690711 , -0.29774258],\n","        [-0.13489895,  0.01090932,  0.3456014 ,  0.2524991 ,\n","         -0.29308322, -0.29094657, -0.6598883 , -0.04186695],\n","        [-0.13489895,  0.01090932,  0.3456014 ,  0.2524991 ,\n","         -0.29308322, -0.29094657, -0.6598883 , -0.04186695]]],\n","      dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"uxxgRGhJfdAj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612754576190,"user_tz":-540,"elapsed":794,"user":{"displayName":"한백규","photoUrl":"","userId":"00157431602964203104"}},"outputId":"70eae3a1-7d64-4dee-b86b-f41b3d455501"},"source":["# Encoder-Decoder Attetnion\n","Q = hidden_dec\n","K = hidden_enc\n","V = hidden_enc\n","\n","attn_out = attention((Q, K, V, enc_dec_mask))\n","attn_out"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 6, 8), dtype=float32, numpy=\n","array([[[-0.13165587,  0.35934266, -0.30766588,  0.4273435 ,\n","          0.10794223,  0.561637  ,  0.7956541 ,  0.28364244],\n","        [-0.20692162,  0.35365945, -0.31671894,  0.46408376,\n","          0.05719474,  0.54531634,  0.786073  ,  0.25542712],\n","        [-0.0051144 ,  0.37713844, -0.29296046,  0.3805099 ,\n","          0.13042648,  0.5581737 ,  0.7983715 ,  0.30378246],\n","        [-0.20495296,  0.35653824, -0.31665277,  0.4680712 ,\n","          0.03770464,  0.5355051 ,  0.7818871 ,  0.24712145],\n","        [ 0.0439067 ,  0.3756832 , -0.28674185,  0.3472341 ,\n","          0.2027991 ,  0.5881414 ,  0.8129915 ,  0.33924115],\n","        [ 0.0439067 ,  0.3756832 , -0.28674185,  0.3472341 ,\n","          0.2027991 ,  0.5881414 ,  0.8129915 ,  0.33924115]]],\n","      dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"y6_0vG8SISFB"},"source":["# Multi Head Attention"]},{"cell_type":"code","metadata":{"id":"3dkSuTsQXJTZ"},"source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","    \"\"\"\n","    Multi Head Attention Class\n","    \"\"\"\n","    def __init__(self, config, name=\"multi_head_attention\"):\n","        \"\"\"\n","        생성자\n","        :param config: Config 객체\n","        :param name: layer name\n","        \"\"\"\n","        super().__init__(name=name)\n","\n","        self.d_model = config.d_model\n","        self.n_head = config.n_head\n","        self.d_head = config.d_head\n","\n","        # Q, K, V input dense layer\n","        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head)\n","        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head)\n","        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head)\n","        # Scale Dot Product Attention class\n","        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n","        # output dense layer\n","        self.W_O = tf.keras.layers.Dense(config.d_model)\n","\n","    def call(self, inputs):\n","        \"\"\"\n","        layer 실행\n","        :param inputs: Q, K, V, attn_mask tuple\n","        :return attn_out: attention 실행 결과\n","        \"\"\"\n","        #########################################\n","        Q, K, V, attn_mask = inputs\n","\n","        Q_m = self.W_Q(Q)\n","        print(Q_m.shape)\n","        Q_m = tf.reshape(Q_m, [-1, tf.shape(Q)[1], self.n_head, self.d_head])\n","        print(Q_m.shape)\n","        Q_m = tf.transpose(Q_m, )\n","        # build multihead Q, K, V\n","        # Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [-1, tf.shape(Q)[1], self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n","        # K_m = tf.transpose(tf.reshape(self.W_K(K), [-1, tf.shape(K)[1], self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n","        # V_m = tf.transpose(tf.reshape(self.W_V(V), [-1, tf.shape(V)[1], self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n","        #########################################"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iYJFgcWCzjka","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612754979051,"user_tz":-540,"elapsed":797,"user":{"displayName":"한백규","photoUrl":"","userId":"00157431602964203104"}},"outputId":"7f8b7726-8b9d-47e5-f49b-2734234bd0dc"},"source":["# Encoder Self Attetnion\n","Q = hidden_enc\n","K = hidden_enc\n","V = hidden_enc\n","\n","attention = MultiHeadAttention(config)\n","attn_out = attention((Q, K, V, enc_self_mask))\n","attn_out"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(1, 4, 8)\n","(1, 4, 2, 4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sD7ckaZWznSu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612754991805,"user_tz":-540,"elapsed":853,"user":{"displayName":"한백규","photoUrl":"","userId":"00157431602964203104"}},"outputId":"b80f5f1e-7a57-4d6f-dc21-4fbb7c15aae5"},"source":["# Decoder Self Attetnion\n","Q = hidden_dec\n","K = hidden_dec\n","V = hidden_dec\n","\n","attn_out = attention((Q, K, V, dec_self_mask))\n","attn_out"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(1, 6, 8)\n","(1, 6, 2, 4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vM9SlWCTzuI0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612754993686,"user_tz":-540,"elapsed":791,"user":{"displayName":"한백규","photoUrl":"","userId":"00157431602964203104"}},"outputId":"072dec46-e0cc-4828-f607-a6e90d1e144d"},"source":["# Encoder-Decoder Attetnion\n","Q = hidden_dec\n","K = hidden_enc\n","V = hidden_enc\n","\n","attn_out = attention((Q, K, V, enc_dec_mask))\n","attn_out"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(1, 6, 8)\n","(1, 6, 2, 4)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"J5TmKt9JIXzc"},"source":["# Feed Forward"]},{"cell_type":"code","metadata":{"id":"1MPtv3zPbxVm"},"source":["class PositionWiseFeedForward(tf.keras.layers.Layer):\n","    \"\"\"\n","    Position Wise Feed Forward Class\n","    \"\"\"\n","    def __init__(self, config, name=\"feed_forward\"):\n","        \"\"\"\n","        생성자\n","        :param config: Config 객체\n","        :param name: layer name\n","        \"\"\"\n","        super().__init__(name=name)\n","\n","        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=tf.nn.relu)\n","        self.W_2 = tf.keras.layers.Dense(config.d_model)\n","\n","    def call(self, inputs):\n","        \"\"\"\n","        layer 실행\n","        :param inputs: inputs\n","        :return ff_val: feed forward 실행 결과\n","        \"\"\"\n","        # linear W_1 and W_2\n","        ff_val = self.W_1(inputs)\n","        ff_val = self.W_2(ff_val)\n","        return ff_val"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vQdqftXNb1al","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612755866912,"user_tz":-540,"elapsed":832,"user":{"displayName":"한백규","photoUrl":"","userId":"00157431602964203104"}},"outputId":"00cdba4c-5109-41dd-d981-0420edc2ffe0"},"source":["# feed-forward class 동작 확인\n","feed_forward = PositionWiseFeedForward(config)\n","ff_val = feed_forward(hidden_enc)\n","ff_val.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([1, 4, 8])"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"uuav6G6Bh8L4"},"source":["# LayerNormal\n","- https://arxiv.org/abs/1607.06450"]},{"cell_type":"code","metadata":{"id":"mzW12eb4h_-4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612755872543,"user_tz":-540,"elapsed":797,"user":{"displayName":"한백규","photoUrl":"","userId":"00157431602964203104"}},"outputId":"980f746b-01d4-4a50-b3ea-fc99d01631d4"},"source":["# 큰 hidden 생성\n","hidden = np.array([[1, 2, 3],\n","                   [11, 11, 13],\n","                   [111, 122, 133]]).astype(np.float32)\n","hidden"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[  1.,   2.,   3.],\n","       [ 11.,  11.,  13.],\n","       [111., 122., 133.]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"MX2ETxuBiVIy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612755874212,"user_tz":-540,"elapsed":836,"user":{"displayName":"한백규","photoUrl":"","userId":"00157431602964203104"}},"outputId":"433daefa-adb2-4488-fdb7-ce12a9f9a4b9"},"source":["# layer_normal 실행\n","layer_norm = tf.keras.layers.LayerNormalization()\n","layer_norm(hidden)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n","array([[-1.2238274 ,  0.        ,  1.2238274 ],\n","       [-0.70670974, -0.70670974,  1.4134184 ],\n","       [-1.2247372 ,  0.        ,  1.2247372 ]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"kDXDxzWjijuo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612755876134,"user_tz":-540,"elapsed":778,"user":{"displayName":"한백규","photoUrl":"","userId":"00157431602964203104"}},"outputId":"697cebfc-d035-423f-81e2-8327a75be9e4"},"source":["# weights\n","layer_norm.get_weights()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array([1., 1., 1.], dtype=float32), array([0., 0., 0.], dtype=float32)]"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"C8TEhPFuiqcj"},"source":["# 평균 값\n","mean = np.mean(hidden, axis=-1, keepdims=True)\n","mean"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YD3GOLr1lV_X"},"source":["# sqrt(var - epsiolo)\n","sigma = np.sqrt(np.var(hidden, axis=-1, keepdims=True) + 0.001)\n","sigma"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RLXZDyCAjAI4"},"source":["# layer normal 계산\n","(hidden - mean) / sigma"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ObWGnfj6Ic_M"},"source":["# Encoder Layer"]},{"cell_type":"code","metadata":{"id":"TOJHTsjinOry"},"source":["class EncoderLayer(tf.keras.layers.Layer):\n","    \"\"\"\n","    Encoder Layer Class\n","    \"\"\"\n","    def __init__(self, config, name='encoder_layer'):\n","        \"\"\"\n","        생성자\n","        :param config: Config 객체\n","        :param name: layer name\n","        \"\"\"\n","        super().__init__(name=name)\n","\n","        self.self_attention = MultiHeadAttention(config)\n","        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.norm_eps)\n","\n","        self.ffn = PositionWiseFeedForward(config)\n","        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.norm_eps)\n","\n","        self.dropout = tf.keras.layers.Dropout(config.dropout)\n"," \n","    def call(self, inputs):\n","        \"\"\"\n","        layer 실행\n","        :param inputs: enc_hidden, self_mask tuple\n","        :return enc_out: EncoderLayer 실행 결과\n","        \"\"\"\n","        enc_hidden, self_mask = inputs\n","        # self attention\n","        self_attn_val = self.self_attention((enc_hidden, enc_hidden, enc_hidden, self_mask))\n","        # add and layer normal\n","        norm1_val = self.norm1(enc_hidden + self.dropout(self_attn_val))\n","        \n","        # feed forward\n","        ffn_val = self.ffn(norm1_val)\n","        # add and layer normal\n","        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n","\n","        return enc_out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"heThi4mtoGhW"},"source":["# EncoderLayer 기능 확인\n","encoder_layer = EncoderLayer(config)\n","enc_out = encoder_layer((hidden_enc, enc_self_mask))\n","enc_out.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MH_M42l5Iezo"},"source":["# Decoder Layer"]},{"cell_type":"code","metadata":{"id":"O2ZJaGlqsxI6"},"source":["class DecoderLayer(tf.keras.layers.Layer):\n","    \"\"\"\n","    Decoder Layer Class\n","    \"\"\"\n","    def __init__(self, config, name='decoder_layer'):\n","        \"\"\"\n","        생성자\n","        :param config: Config 객체\n","        :param name: layer name\n","        \"\"\"\n","        super().__init__(name=name)\n","\n","        self.self_attention = MultiHeadAttention(config)\n","        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.norm_eps)\n","\n","        self.ende_attn = MultiHeadAttention(config)\n","        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.norm_eps)\n","\n","        self.ffn = PositionWiseFeedForward(config)\n","        self.norm3 = tf.keras.layers.LayerNormalization(epsilon=config.norm_eps)\n","\n","        self.dropout = tf.keras.layers.Dropout(config.dropout)\n","\n","    def call(self, inputs):\n","        \"\"\"\n","        layer 실행\n","        :param inputs: dec_hidden, enc_out, self_mask, ende_mask tuple\n","        :return dec_out: DecoderLayer 실행 결과\n","        \"\"\"\n","        dec_hidden, enc_out, self_mask, ende_mask = inputs\n","        # self attention\n","        self_attn_val = self.self_attention((dec_hidden, dec_hidden, dec_hidden, self_mask))\n","        # add and layer normal\n","        norm1_val = self.norm1(dec_hidden + self.dropout(self_attn_val))\n","\n","        # encoder and decoder attention\n","        ende_attn_val = self.ende_attn((norm1_val, enc_out, enc_out, ende_mask))\n","        # add and layer normal\n","        norm2_val = self.norm2(norm1_val + self.dropout(ende_attn_val))\n","\n","        # feed forward\n","        ffn_val = self.ffn(norm2_val)\n","        # add and layer normal\n","        dec_out = self.norm3(norm2_val + self.dropout(ffn_val))\n","\n","        return dec_out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TEAxwUw9tGDk"},"source":["# Decoder 실행\n","decoder_layer = DecoderLayer(config)\n","dec_out = decoder_layer((hidden_dec, hidden_enc, dec_self_mask, enc_dec_mask))\n","dec_out.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z4OCTlKPIh0Z"},"source":["# Weight Shared Embedding"]},{"cell_type":"code","metadata":{"id":"mjoffNaW4J_W"},"source":["class SharedEmbedding(tf.keras.layers.Layer):\n","    \"\"\"\n","    Weighed Shaed Embedding Class\n","    \"\"\"\n","    def __init__(self, config, name='weight_shared_embedding'):\n","        \"\"\"\n","        생성자\n","        :param config: Config 객체\n","        :param name: layer name\n","        \"\"\"\n","        super().__init__(name=name)\n","\n","        self.n_vocab = config.n_vocab\n","        self.d_model = config.d_model\n","    \n","    def build(self, input_shape):\n","        \"\"\"\n","        shared weight 생성\n","        :param input_shape: Tensor Shape (not used)\n","        \"\"\"\n","        with tf.name_scope('shared_embedding_weight'):\n","            self.shared_weights = self.add_weight(\n","                'weights',\n","                shape=[self.n_vocab, self.d_model],\n","                initializer=tf.keras.initializers.TruncatedNormal(stddev=self.d_model ** -0.5)\n","            )\n","\n","    def call(self, inputs, mode='embedding'):\n","        \"\"\"\n","        layer 실행\n","        :param inputs: 입력\n","        :param mode: 실행 모드\n","        :return: embedding or linear 실행 결과\n","        \"\"\"\n","        # mode가 embedding일 경우 embedding lookup 실행\n","        if mode == 'embedding':\n","            return self._embedding(inputs)\n","        # mode가 linear일 경우 linear 실행\n","        elif mode == 'linear':\n","            return self._linear(inputs)\n","        # mode가 기타일 경우 오류 발생\n","        else:\n","            raise ValueError(f'mode {mode} is not valid.')\n","    \n","    def _embedding(self, inputs):\n","        \"\"\"\n","        embedding lookup\n","        :param inputs: 입력\n","        \"\"\"\n","        # lookup by gather\n","        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n","        # muliply d_model ** 0.5\n","        embed *= self.d_model ** 0.5\n","        return embed\n","\n","    def _linear(self, inputs):  # (bs, n_seq, d_model)\n","        \"\"\"\n","        linear 실행\n","        :param inputs: 입력\n","        \"\"\"\n","        # matmul inputs, shared_weights (transpose_b=True)\n","        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n","        return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1kFRtg0U7o5W"},"source":["embedding = SharedEmbedding(config)\n","hidden_dec = embedding(train_dec_inputs)\n","hidden_dec.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ttHI4hYM7os_"},"source":["linear_outputs = embedding(hidden_dec, mode=\"linear\")\n","linear_outputs.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zv8egauKIl6h"},"source":["# Postional Encoding"]},{"cell_type":"code","metadata":{"id":"IfIQaEslDFPP"},"source":["class PositionalEmbedding(tf.keras.layers.Layer):\n","    \"\"\"\n","    Positional Embedding Class\n","    \"\"\"\n","    def __init__(self, config, name='position_embedding'):\n","        \"\"\"\n","        생성자\n","        :param config: Config 객체\n","        :param name: layer name\n","        \"\"\"\n","        super().__init__(name=name)\n","        \n","        pos_encoding = PositionalEmbedding.get_sinusoid_encoding(config.n_seq, config.d_model)\n","        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, trainable=False, weights=[pos_encoding])\n","\n","    def call(self, inputs):\n","        \"\"\"\n","        layer 실행\n","        :param inputs: 입력\n","        :return embed: positional embedding lookup 결과\n","        \"\"\"\n","        # make position (0...n_seq)\n","        position = tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True)\n","        position = tf.cast(position, tf.int32)\n","        # embedding lookup\n","        embed = self.embedding(position)\n","        return embed\n","\n","    @staticmethod\n","    def get_sinusoid_encoding(n_seq, d_model):\n","        \"\"\"\n","        sinusoid encoding 생성\n","        :param n_seq: sequence number\n","        :param n_seq: model hidden dimension\n","        :return: positional encoding table\n","        \"\"\"\n","        # calculate angle\n","        exs = [2 * (i_ang // 2) / d_model for i_ang in range(d_model)]\n","        angles = [np.power(10000, ex) for ex in exs]\n","        # calculate position\n","        pos_encoding = np.array([[pos / angle for angle in angles] for pos in range(n_seq)])\n","        # sin even number\n","        pos_encoding[:, 0::2] = np.sin(pos_encoding[:, 0::2])\n","        # cos odd number\n","        pos_encoding[:, 1::2] = np.cos(pos_encoding[:, 1::2])\n","        return tf.cast(pos_encoding, tf.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9P_eFKxH26ei"},"source":["# position encoding 확인\n","pos_encoding = PositionalEmbedding.get_sinusoid_encoding(4, 4)\n","pos_encoding"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bDMcqbPp3ipc"},"source":["# display\n","plt.pcolormesh(pos_encoding, cmap='RdBu')\n","plt.xlabel('Depth')\n","plt.xlim((0, config.d_model))\n","plt.ylabel('Position')\n","plt.colorbar()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5uuUYZeeDWjc"},"source":["# PositionalEmbedding 클래스 시험\n","pos_embedding = PositionalEmbedding(config)\n","dec_pos = pos_embedding(train_enc_inputs)\n","dec_pos.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yzu-44yIEfEO"},"source":["# 512x512 position encoding table 생성\n","pos_encoding = PositionalEmbedding.get_sinusoid_encoding(512, 512)\n","# display\n","plt.pcolormesh(pos_encoding, cmap='RdBu')\n","plt.xlabel('Depth')\n","plt.xlim((0, 512))\n","plt.ylabel('Position')\n","plt.colorbar()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sgmPfzaBIo_V"},"source":["# Transformer"]},{"cell_type":"code","metadata":{"id":"NjUj8dKbLBPV"},"source":["class Transformer(tf.keras.Model):\n","    \"\"\"\n","    Transformer Class\n","    \"\"\"\n","    def __init__(self, config, name='transformer'):\n","        \"\"\"\n","        생성자\n","        :param config: Config 객체\n","        :param name: layer name\n","        \"\"\"\n","        super().__init__(name=name)\n","\n","        self.i_pad = config.i_pad\n","        self.embedding = SharedEmbedding(config)\n","        self.position = PositionalEmbedding(config)\n","        \n","        self.encoder_layers = [EncoderLayer(config, name=f'encoder_layer_{i}') for i in range(config.n_layer)]\n","        self.decoder_layers = [DecoderLayer(config, name=f'decoder_layer_{i}') for i in range(config.n_layer)]\n","\n","        self.dropout = tf.keras.layers.Dropout(config.dropout)\n","\n","    def call(self, inputs):\n","        \"\"\"\n","        layer 실행\n","        :param inputs: enc_tokens, dec_tokens tuple\n","        :return logits: dec_tokens에 대한 다음 토큰 예측 결과 logits\n","        \"\"\"\n","        enc_tokens, dec_tokens = inputs\n","        # encoder self attention mask\n","        enc_self_mask = get_pad_mask(enc_tokens, self.i_pad)\n","        # decoder self attention mask\n","        dec_self_mask = get_causal_mask(dec_tokens, self.i_pad)\n","        # encoder and decoder attention mask\n","        enc_dec_mask = get_pad_mask(enc_tokens, self.i_pad)\n","\n","        # enc_tokens, dec_tokens embedding lookup\n","        enc_embed = self.get_embedding(enc_tokens)\n","        dec_embed = self.get_embedding(dec_tokens)\n","\n","        #########################################\n","        #########################################\n","\n","        # call weight shared embedding (model=linear)\n","        logits = self.embedding(dec_hidden, mode='linear')\n","        return logits\n","    \n","    def get_embedding(self, tokens):\n","        \"\"\"\n","        token embedding, position embedding lookup\n","        :param tokens: 입력 tokens\n","        :return embed: embedding 결과\n","        \"\"\"\n","        embed = self.embedding(tokens) + self.position(tokens)\n","        return embed"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pbcZGCC2L-MB"},"source":["# Transformer 기능 확인. 최종 결과가 (bs, n_seq(dec), n_vocab)\n","transformer = Transformer(config)\n","logits = transformer((train_enc_inputs, train_dec_inputs))\n","logits.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lGaoO7ckM0XM"},"source":[""],"execution_count":null,"outputs":[]}]}